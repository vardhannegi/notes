# ðŸ§  Apache Spark Notes (Corrected)

## Spark Architecture Overview

- `sparkContext` is the point of contact between the driver program and the cluster manager.
- The **cluster manager** (e.g., YARN, Mesos, Kubernetes) launches the **Application Master container (AMC)** on the driver node.
- Inside the driver, the JVM runs the main application logic (`application driver` for Scala/Java, `pyspark driver` for Python). PySpark communicates with the JVM using **Py4J**, converting Python code into JVM-executable instructions.
- **Worker nodes** also run JVMs to execute Spark tasks and must have Python installed if **UDFs (User Defined Functions)** are used. However, **UDFs are discouraged** as they increase the load on workers and can hinder performance due to serialization/deserialization overhead.

> Note: **JVM-based execution is gradually being replaced by C++-based executors**, such as **Databricks Photon** and **Apache Fabric**, for improved performance. However, JVM will remain relevant for a long time.

- On **Databricks**, a `SparkSession` is automatically created; manual initialization is not required.

---

## Lazy Evaluation & Spark Jobs

- Spark uses **lazy evaluation**, meaning transformations like `.filter()`, `.groupBy()`, `.agg()` are **not executed immediately**.
- These transformations are added to a **logical plan**, which Spark **optimizes** (e.g., reorders operations, removes unnecessary steps).
- The optimized plan is only **executed** when an **action** like `.show()`, `.count()`, `.collect()`, or `.display()` is triggered.
- You can inspect the execution plan using `.explain()`. Read it from bottom-up to understand execution flow.

---

## RDDs (Resilient Distributed Datasets)

- **RDD** is a fundamental, immutable distributed data structure in Spark, like `list`, `dict`, or `tuple` in Python.
- We usually work with **DataFrames**, which Spark converts to RDDs behind the scenes for distributed processing.
- RDDs are **resilient** because they are fault-tolerant. If an RDD is lost, Spark rebuilds it using the **DAG (Directed Acyclic Graph)** lineage of transformations.

Example:  
- `df1 â†’ Transformation-1 â†’ RDD1`  
- `df1 â†’ Transformation-2 â†’ RDD2`  
- If RDD2 fails, Spark uses RDD1 and the DAG to recreate it.

---

## Narrow vs Wide Transformations

- **Narrow Transformations**: Each input partition contributes to only one output partition. (e.g., `filter`, `select`)
- **Wide Transformations**: Data must be shuffled across the network. (e.g., `groupBy`, `join`)
  - To make wide transformations efficient, Spark **re-partitions data** based on keys (e.g., all Delhi rows in one partition).
  - Default shuffle partitions = **200**, which may be excessive for small datasets.

---

## Partitioning and Repartitioning

- **Default block size** (e.g., HDFS, Delta): ~128 MB.
- Use `repartition(n)` to increase partitions (causes shuffle).
- Use `coalesce(n)` to reduce partitions (avoids shuffle).

---

## Jobs, Stages, and Tasks

- One **action** triggers a **job**, which has at least **one stage**.
- Each **stage** may contain multiple **tasks** (one per partition).
- Narrow transformations stay in a single stage; wide transformations create new stages.

---

## Adaptive Query Execution (AQE)

- **AQE** optimizes execution at runtime:
  - Coalesces 200 shuffle partitions down to the needed number (e.g., 2).
  - Chooses optimal **join strategy** dynamically.
  - Helps especially with **skewed data** or uneven partitions.

---

## Join Strategies in Spark

- **Sort-Merge Join (default)**: Efficient for large, sorted datasets.
- **Shuffle Hash Join**: Preferred for smaller datasets where one side fits in memory.
- AQE can switch between these based on runtime stats.

---

## Logical to Physical Plan Conversion

1. **Unresolved Logical Plan**
2. **Resolved Logical Plan**
3. **Optimized Logical Plan**
4. **Physical Plans**
5. **Cost Model**
6. **Execution by Executors**

---

## Memory Management

### Driver Memory

- Includes **JVM Heap Memory** and **Overhead Memory**.
- Overhead = `max(10% of heap, 384 MB)`.
- **Driver OOM** occurs if data collected from executors (e.g., using `.collect()`) exceeds available memory.

### Executor Memory

- Consists of:
  - **JVM Heap Memory**
  - **Off-Heap Memory**
  - **Overhead Memory**
  - **PySpark Memory**

- JVM Heap Memory split:
  - **Reserved Memory** (300 MB fixed)
  - **User Memory** (~40% by default)
  - **Spark Memory Pool (Unified Memory Management)** (~60%)

#### Unified Memory Management (UMM)

- Spark Memory Pool splits into:
  - **Storage Memory**: For caching (`df.cache()`)
  - **Execution Memory**: For transformations

- Spark can **borrow memory** from storage to execution and vice versa.
- If needed, **eviction happens in LRU (Least Recently Used)** fashion.

---

## Executor OOM & Data Skew

- For normally distributed data: If a group is too large for memory, Spark spills processed partitions to **disk** (data spill).
- For **skewed data**: If one group dominates (e.g., 80% of total), it may not be splittable â†’ causes **OOM**.

### Solution: Salting

- Add a `salt` column with random values (e.g., 1 to 4).
- Instead of one 1.5 GB partition, salt splits it into 4 manageable partitions (~375 MB each).
- Helps in **skew mitigation**.

---

## Storage Levels

- Storage levels in `.persist()` or `.cache()` control where and how data is stored:
  - `MEMORY_AND_DISK` (default in `df.cache()`)
  - `MEMORY_ONLY`
  - `DISK_ONLY`
  - `MEMORY_ONLY_2` (replicated)
  - `OFF_HEAP`

---

## Edge Node

- An **edge node** is a client node (often your local machine) that interacts with the **Resource Manager**.
- The RM then creates a driver inside this client. Commonly used in development, **not recommended for production**.

---

## Partition Pruning

- **Partition pruning** is a performance optimization.
- It saves data in directory structures **based on column values**, e.g., `/year=2024/month=06/`.
- Queries with filters on partitioned columns skip unnecessary files.
